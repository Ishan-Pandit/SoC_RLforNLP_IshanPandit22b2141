{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> Monte Carlo Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_policy_evaluation(env, policy, episodes, gamma=1.0):\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    returns = {state: [] for state in range(env.observation_space.n)}\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        G = 0\n",
    "        for step in reversed(episode):\n",
    "            state, action, reward = step\n",
    "            G = gamma * G + reward\n",
    "            if not any(s == state for s, _, _ in episode[:episode.index(step)]):\n",
    "                returns[state].append(G)\n",
    "                value_table[state] = np.mean(returns[state])\n",
    "                \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.random.choice(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "episodes = 5000\n",
    "value_table_mc = mc_policy_evaluation(env, random_policy, episodes)\n",
    "\n",
    "plt.plot(value_table_mc)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Monte Carlo Value Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> TD Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_policy_evaluation(env, policy, episodes, alpha=0.1, gamma=1.0):\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            value_table[state] += alpha * (reward + gamma * value_table[next_state] - value_table[state])\n",
    "            state = next_state\n",
    "            \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_table_td = td_policy_evaluation(env, random_policy, episodes)\n",
    "\n",
    "plt.plot(value_table_td)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.title('TD Value Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> Comparing Cumulative Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(env, policy, episodes, algo):\n",
    "    rewards = []\n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        cumulative_reward += episode_reward\n",
    "        rewards.append(cumulative_reward)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5000\n",
    "mc_rewards = simulate(env, random_policy, episodes, \"MC\")\n",
    "td_rewards = simulate(env, random_policy, episodes, \"TD\")\n",
    "\n",
    "plt.plot(mc_rewards, label='Monte Carlo')\n",
    "plt.plot(td_rewards, label='TD Learning')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward vs Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.1, 0.5, 0.9]\n",
    "gammas = [0.9, 0.95, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    value_table_td = td_policy_evaluation(env, random_policy, episodes, alpha=alpha)\n",
    "    plt.plot(value_table_td, label=f'alpha={alpha}')\n",
    "\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.title('TD Value Function with different alpha values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in gammas:\n",
    "    value_table_td = td_policy_evaluation(env, random_policy, episodes, gamma=gamma)\n",
    "    plt.plot(value_table_td, label=f'gamma={gamma}')\n",
    "\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.title('TD Value Function with different gamma values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
